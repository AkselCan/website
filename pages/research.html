---
permalink: "research/"
layout: 'layouts/page.html'
title: 'Our Research'
---

<br><br><br>
<p style="color:black; padding-left: 4%; padding-right: 10%; font-size: 15px;">

  Our research investigates the solution of replacing LiDARS with
  stereo cameras, and the sensor fusion of semantic information extracted
  from monocular cameras and depth information from stereo cameras for increased
  autonomous accuracy.

  Our project idea provides a vision-based perception platform, extended to multi-vehicles
  with the ability of sharing information between fleets of vehicles,
  such that the map and localization at intersections are fused to achieve improved coverage.

</p>

<!-- Project 1 -->
<div class="research-container">
  <div class="research-col">
    <h1> Multi-agent System for non-Holonomic Racing (MuSHR) Car </h1>
    <p>
      <br>
      Our current project focuses on localization and mapping for autonomous navigation
      utilizing MUSHr hardware. We look forward to transferring this software from our assembled MUSHr
      racecar to future larger projects. Using SLAM (Simultaneous Localization and Mapping) on
      a simulation of the racecar and its surroundings, we plan on later implementing this software
      onto the physical car and testing it in the physical world.

      <br> <br> Pictured on the right is our first assembled and operational MUSHr racecar. <br><br>

      Our next step is to implement a simulation using SLAM that allows the vehicle to gather data using
      the LiDAR and stereo camera installed on-board for accurate and consistent localization. Our goals is for our
      car to physically showcase fully autonomous navigation.
    </p>
  </div>
  <div class="research-col">
    <img src="../images/cars.png" alt="image of a MUSHr racecar">
  </div>
</div> <!--Project 1 -->

<!--Project 2 -->
<div class="research-container">
  <div class="research-col-1">
    <img src="../images/voxl.png" alt="holder image">
  </div>
  <div class="research-col-1">
    <h1> Autonomous Drones: PANTHER (Perception-Aware Trajectory Planner in Dynamic Environments) </h1>
    <p>
      <br>
      Another project we are looking forward to implementing is building an autonomous drone. This project is
      based on previous work, PANTHER: Perception-Aware Trajectory Planner in Dynamic Environments. Currently, this
      project is still in its beginning stages of assembly, but future goals include implementing SLAM and
      path planning algorithms to autonomously navigate aerially.
    </p>
    <p> Pictured on the left is the VOXL and power source that will be installed on Air Lab's drone.
    </p>
  </div>
</div>
<!--Project 2-->

<!-- Project 3 -->
<div class="research-container">
  <div class="research-col">
    <h1> Autonomous Rovers: Clearpath Husky/Jackal and Trodden Robotics Rover </h1>
    <p>
      <br>
      The third project we are interested in involves implementing our software onto larger
      vehicles, such as a rover. Specifically, we are interested in utilizing hardware from the Clearpath Husky/Jackal
      and
      Trodden Robotics Rover.

      Our goal is to autonomously navigate an environment, like the MUSHr racecars and drone projects,
      but with larger vehicles. Currently, we are only purchasing the parts and planning the assembly of
      the rover as we focus on the previously mentioned two projects first.
    </p>
  </div>
  <div class="research-col">
    <img src="../images/mushr-car.png" alt="image of a MUSHr racecar">
  </div>
</div> <!--Project 3 -->


<div class="research-container">
  <div class="research-col-1">
    <img src="../images/real-sense-camera.png" alt="holder image">
  </div>
  <div class="research-col-1">
    <h1> 3D Perception with Stereo Cameras </h1>
    <p>
      <br>
      We currently plan to utilize a RealSense Depth Camera D435i to capture the depth
      and geometric information of the vehicle's surroundings during localization.
      Stereo cameras feature two lenses that simulate the human visual experience through
      triangulation and captures the depth perception that a monocular camera cannot.
      In parallel to our project, we are looking into extracting the depth information
      from the stereo camera and generating a point cloud similar to a LiDAR point cloud.
      This will be used as a replacement since LiDAR is typically more expensive.
    </p>
  </div>
</div>


<!--
<div class="research-container">
    <div class="research-col">
        <br><br><br>
        <h1> Progress </h1>
        <p>
            <br>
            In tangent to our current working MUSHr racecar, we are also assembling others 
            MUSHr racecars and have started building a drone so that we can extend our research 
            to aerial vehicles. These vehicles will be equipped with sensors such as stereo cameras 
            and LiDARs for increased accuracy in environment perception.  
        </p>
    </div>
    <div class="research-col">
        <img src="../images/assembled-cars.png" alt="image of a MUSHr racecar">
    </div>
</div>
-->

<!-- Previous Work -->
<div class="research-container">
  <div class="research-col">
    <h1> Previous Work </h1>
    <p>
      <br>
      We present a Similarity-based Incremental Learning Algorithm
      (SILA) [1] for pedestrian motion prediction with the ability of
      improving the learned model over the time as data is obtained
      incrementally.
      To keep the model size efficient, the motion primitives learned
      from the new data are compared with the previously known ones,
      and similar motion primitives are fused while novel motion
      primitives are added to the model.
      <br> <br>
      SimFuse is a similarity-based model fusion algorithm for
      improving the prediction accuracy which enables autonomous
      agents to incrementally update their knowledge by communicating
      with other vehicles (V2V) or by infrastructures (V2I).
      <br> <br>
      This is where we propose the use of a multi-camera visual SLAM
      method, capable of gauging depth through machine learning.
    </p>
  </div>
  <div class="research-col">
    <img src="../images/graph-1.png" alt="graph 1">
  </div>
</div>


<div class="research-container">
  <div class="research-col-1">
    <img src="../images/graph-2.png" alt="graph-2">
  </div>
  <div class="research-col-1">
    <p>
      <br><br>
      By merging the maps created from each camera, effectively
      creating a multi-camera system, one could hope to remedy
      complications found in single camera systems, such as
      environmental conditions, like the affect of direct sunlight or other
      obscuring weather conditions.
      <br> <br> <br>
    </p>
    <h1> References </h1>
    <p> [1] G. Habibi, N. Jaipuria and J. P. How, "SILA: An
      Incremental Learning Approach for Pedestrian
      Trajectory Prediction," 2020 IEEE/CVF
      Conference on Computer Vision and Pattern
      Recognition Workshops (CVPRW), 2020, pp. 4411-
      4421, doi: 10.1109/CVPRW50498.2020.00520. </p>

    <p> [2] G. Habibi and J. P. How, "Human Trajectory Prediction Using Similarity-Based
      Multi-Model Fusion," in IEEE Robotics and Automation Letters, vol. 6, no. 2, pp.
      715-722, April 2021, doi: 10.1109/LRA.2020.3048652.
    </p>
  </div>
</div>
<!-- Previous Work -->


<br><br><br>
<h1 style="text-align: center;"> Recent Research Presentation Poster Submissions </h1><br><br>
<div class="center-img">
  <!--<img src="../images/daniel-poster.png" style="width: 40%; align-content: center;"> 
    <img src="../images/yuki-tyler-poster.png" style="width: 40%; align-content: center;"> -->
  <img src="../images/Airou-research-poster.jpg" alt="Research poster" style="width: 80%; align-content: center;">
</div>


<br><br><br><br>
<!-------- Research content -------------->